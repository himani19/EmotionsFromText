{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or\n",
      "[nltk_data]     service not known>\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn import preprocessing\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6839, 13)\n",
      "                                              Phrase\n",
      "0  [Worrying is a down payment on a problem you m...\n",
      "1  Whatever you decide to do make sure it makes y...\n",
      "2  @Max_Kellerman  it also helps that the majorit...\n",
      "3                                  George S. Patton \n",
      "4  My roommate: it's okay that we can't spell bec...\n",
      "(6839, 11)\n"
     ]
    }
   ],
   "source": [
    "#read the data set\n",
    "data = pd.read_csv(\"./Data/multilabel_emotion_dataset.csv\",sep=\"\\t\")\n",
    "print data.shape\n",
    "# data = pd.read_csv(\"./Data/phrases.csv\")\n",
    "# data=pd.DataFrame(data)\n",
    "# print type(data)\n",
    "# print data.shape\n",
    "# data=data.dropna()\n",
    "data_phrase=pd.DataFrame()\n",
    "data_phrase['Phrase']=data['Tweet']\n",
    "print data_phrase.head()\n",
    "# print type(data_phrase)\n",
    "df_emo=data\n",
    "df_emo=df_emo.drop([\"ID\",\"Tweet\"],axis=1)\n",
    "data_emotion=df_emo.values\n",
    "print data_emotion.shape\n",
    "\n",
    "# print data_phrase\n",
    "# print data['Emotion'].unique()\n",
    "# print data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [1 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print data_emotion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data set via cleaning, tokenisation and lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Phrase\n",
      "0  Worrying is a down payment on a problem you ma...\n",
      "1  Whatever you decide to do make sure it makes y...\n",
      "2  USER it also helps that the majority of NFL co...\n",
      "3                                    George S Patton\n",
      "4  My roommate it s okay that we can t spell beca...\n"
     ]
    }
   ],
   "source": [
    "# print data\n",
    "import re\n",
    "\n",
    "def clean_dataset(data_cf):\n",
    "    translator = string.maketrans('', '')\n",
    "    l=[]\n",
    "    for index,row in data_cf.iterrows():\n",
    "        row['Phrase'] = row['Phrase'].replace('[','')\n",
    "        row['Phrase'] = row['Phrase'].replace(']','')\n",
    "        row['Phrase'] = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','URL', row['Phrase'])\n",
    "        row['Phrase'] = re.sub('@[^\\s]+','USER', row['Phrase'])\n",
    "#         row['Phrase'] = row['Phrase'].lower().replace(\"ё\", \"е\")\n",
    "        row['Phrase'] = re.sub('[^a-zA-Zа-яА-Я1-9]+', ' ', row['Phrase'])\n",
    "        row['Phrase'] = re.sub(' +',' ', row['Phrase'])\n",
    "        row['Phrase'] = row['Phrase'].strip()\n",
    "        row['Phrase']=re.sub(r'([^\\s\\w]|_)+', '', row['Phrase'])\n",
    "        l.append(row['Phrase'])\n",
    "#         print row['Phrase']\n",
    "        \n",
    "#         row['Phrase'] = row['Phrase'].translate(translator,string.punctuation)\n",
    "    data_cf['Phrase']=l\n",
    "    return data_cf\n",
    "data_clean = clean_dataset(data_phrase)\n",
    "print data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worrying payment problem may never joyce meyer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whatever decide make sure makes happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user also helps majority nfl coaching inept bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>george patton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roommate okay spell autocorrect terrible first...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase\n",
       "0  worrying payment problem may never joyce meyer...\n",
       "1              whatever decide make sure makes happy\n",
       "2  user also helps majority nfl coaching inept bi...\n",
       "3                                      george patton\n",
       "4  roommate okay spell autocorrect terrible first..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenise(data):\n",
    "    ## Convert words to lower case and split them    \n",
    "    ## Remove stop words\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    l=[]\n",
    "    for index,row in data.iterrows():\n",
    "        text = str(row['Phrase']).lower().split(' ')\n",
    "        text = [w.strip() for w in text if not w in stops and len(w) >= 2]\n",
    "        text = \" \".join(text)\n",
    "        row['Phrase'] = text\n",
    "        l.append(row['Phrase'])\n",
    "    # split the dataset into tokens\n",
    "    data['Phrase']=l\n",
    "    return data\n",
    "data_token = tokenise(data_clean)\n",
    "data_token.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>worri payment problem may never joyc meyer mot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>whatev decid make sure make happi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user also help major nfl coach inept bill brie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>georg patton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>roommat okay spell autocorrect terribl firstwo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase\n",
       "0  worri payment problem may never joyc meyer mot...\n",
       "1                  whatev decid make sure make happi\n",
       "2  user also help major nfl coach inept bill brie...\n",
       "3                                       georg patton\n",
       "4  roommat okay spell autocorrect terribl firstwo..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatization(dataset):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    l=[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        text = str(row['Phrase']).split()\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "#         list_of_words.append(stemmed_words)\n",
    "        text = \" \".join(stemmed_words)\n",
    "        row['Phrase'] = text\n",
    "        l.append(row['Phrase'])\n",
    "        \n",
    "    data['Phrase'] = l\n",
    "    return dataset\n",
    "data_stemmed = lemmatization(data_token)\n",
    "data_stemmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6839, 9611)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bag_of_words(data):\n",
    "    count_vect=CountVectorizer(encoding='latin-1')\n",
    "    count_vect.fit(data) #creates vocab of words\n",
    "#     print (count_vect.vocabulary_)\n",
    "    data_phrase=count_vect.transform(data)\n",
    "#     print(data_phrase.shape)\n",
    "#     print(type(data_phrase))\n",
    "#     print(data_phrase.toarray())\n",
    "    return data_phrase,count_vect\n",
    "data_phrase,count_vect=bag_of_words(data['Phrase'])\n",
    "data_phrase.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6839, 9611)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tf_idf(data_phrase):\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    phrase_tfidf = tfidf_transformer.fit_transform(data_phrase)\n",
    "    return phrase_tfidf.toarray(),tfidf_transformer\n",
    "phrase_tfidf,tfidf_transformer=tf_idf(data_phrase)\n",
    "phrase_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import LabelEncoder\n",
    "# #creating labelEncoder\n",
    "# le = preprocessing.LabelEncoder()\n",
    "# # Converting string labels into numbers.\n",
    "# emotion_data=data['Emotion'].values\n",
    "# emo_enc=le.fit_transform(emotion_data)\n",
    "# print emo_enc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (5471, 9611) (1368, 9611)\n",
      "label (5471, 11) (1368, 11)\n"
     ]
    }
   ],
   "source": [
    "train_phrase, validate_phrase = np.split(phrase_tfidf,[int(.8*len(phrase_tfidf))])\n",
    "# train_label,validate_label=np.split(emotion_data,[int(.8*len(emotion_data))])\n",
    "train_label,validate_label=np.split(data_emotion,[int(.8*len(data_emotion))])\n",
    "print \"data\",train_phrase.shape,validate_phrase.shape\n",
    "print \"label\",train_label.shape,validate_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Sklearn's score on training data :\", 0.3555108755254981)\n",
      "(\"Sklearn's score on testing data :\", 0.12938596491228072)\n",
      "Classification report for testing data :-\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.55      0.59       494\n",
      "           1       0.34      0.10      0.15       209\n",
      "           2       0.61      0.52      0.56       524\n",
      "           3       0.61      0.40      0.49       272\n",
      "           4       0.67      0.54      0.60       492\n",
      "           5       0.38      0.26      0.31       134\n",
      "           6       0.58      0.40      0.47       421\n",
      "           7       0.24      0.13      0.17       160\n",
      "           8       0.50      0.40      0.45       394\n",
      "           9       0.44      0.16      0.23        69\n",
      "          10       0.13      0.03      0.05        63\n",
      "\n",
      "   micro avg       0.57      0.41      0.48      3232\n",
      "   macro avg       0.47      0.32      0.37      3232\n",
      "weighted avg       0.55      0.41      0.47      3232\n",
      " samples avg       0.52      0.42      0.44      3232\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/himani/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/himani/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in samples with no true labels.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "modelknn = KNeighborsClassifier(n_neighbors=3)\n",
    "modelknn.fit(train_phrase, train_label)\n",
    "label_predicted = modelknn.predict(validate_phrase)\n",
    " \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "sklearn_score_train = modelknn.score(train_phrase,train_label)\n",
    "print(\"Sklearn's score on training data :\",sklearn_score_train)\n",
    "sklearn_score_test = modelknn.score(validate_phrase,validate_label)\n",
    "print(\"Sklearn's score on testing data :\",sklearn_score_test)\n",
    "print(\"Classification report for testing data :-\")\n",
    "print(classification_report(validate_label, label_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict={0:\"anger\",1:\"anticipation\",2:\"disgust\",3:\"fear\",4:\"joy\",5:\"love\",6:\"optimism\",7:\"pessimism\",8:\"sadness\",9:\"surprise\",10:\"trust\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter text:I will kill you\n",
      "anger\n",
      "disgust\n"
     ]
    }
   ],
   "source": [
    "test_sentence=raw_input(\"Enter text:\")\n",
    "test_sen=[]\n",
    "test_sen.append(test_sentence)\n",
    "bow=count_vect.transform(test_sen)\n",
    "tf_test=tfidf_transformer.transform(bow).toarray()\n",
    "predicted_test= modelknn.predict(tf_test)\n",
    "# le.inverse_transform(predicted_test)\n",
    "for i in range(0,len(predicted_test[0])):\n",
    "    if(predicted_test[0][i]==1):\n",
    "        print mapping_dict[i]\n",
    "# print predicted_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
